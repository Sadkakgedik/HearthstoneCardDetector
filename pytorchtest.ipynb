{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/10, Batch 0/1, Loss: 1.0599, Training Accuracy: 72.16%\n",
      "Epoch 0/10, Validation Accuracy: 66.48%\n",
      "Epoch 1/10, Batch 0/1, Loss: 0.8849, Training Accuracy: 66.48%\n",
      "Epoch 1/10, Validation Accuracy: 66.48%\n",
      "Epoch 2/10, Batch 0/1, Loss: 0.7996, Training Accuracy: 66.48%\n",
      "Epoch 2/10, Validation Accuracy: 66.48%\n",
      "Epoch 3/10, Batch 0/1, Loss: 0.8090, Training Accuracy: 66.48%\n",
      "Epoch 3/10, Validation Accuracy: 66.48%\n",
      "Epoch 4/10, Batch 0/1, Loss: 0.7856, Training Accuracy: 66.48%\n",
      "Epoch 4/10, Validation Accuracy: 66.48%\n",
      "Epoch 5/10, Batch 0/1, Loss: 0.7884, Training Accuracy: 66.48%\n",
      "Epoch 5/10, Validation Accuracy: 66.48%\n",
      "Epoch 6/10, Batch 0/1, Loss: 0.7886, Training Accuracy: 66.48%\n",
      "Epoch 6/10, Validation Accuracy: 66.48%\n",
      "Epoch 7/10, Batch 0/1, Loss: 0.7768, Training Accuracy: 66.48%\n",
      "Epoch 7/10, Validation Accuracy: 66.48%\n",
      "Epoch 8/10, Batch 0/1, Loss: 0.7633, Training Accuracy: 66.48%\n",
      "Epoch 8/10, Validation Accuracy: 66.48%\n",
      "Epoch 9/10, Batch 0/1, Loss: 0.7546, Training Accuracy: 66.48%\n",
      "Epoch 9/10, Validation Accuracy: 66.48%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the neural network model\n",
    "class CardDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CardDetector, self).__init__()\n",
    "        # Define the convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        # Define the pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # Define the fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, 3)\n",
    "    \n",
    "    # Define the forward pass of the neural network\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the neural network model\n",
    "model = CardDetector()\n",
    "\n",
    "# Define the image transformations to be applied to the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # Resize the images to 64x64 pixels\n",
    "    transforms.ToTensor(),  # Convert the images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize the pixel values to have mean=0.5 and std=0.5\n",
    "])\n",
    "\n",
    "# Load the training dataset and create a dataloader\n",
    "train_dataset = datasets.ImageFolder(\"train/\", transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=392, shuffle=True)\n",
    "\n",
    "# Load the validation dataset and create a dataloader\n",
    "val_dataset = datasets.ImageFolder(\"val/\", transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "# Define the loss function to be used during training\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer to be used during training\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        # Set the model to training mode\n",
    "        model.train()\n",
    "\n",
    "        # Zero out the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    print(f\"Epoch {epoch}/{num_epochs}, Batch {i}/{len(train_loader)}, Loss: {loss.item():.4f}, Training Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    # Switch to evaluation mode and calculate accuracy on validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Validation Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    with open('model_state.pt','wb') as f:\n",
    "        torch.save(model.state_dict(),f)\n",
    "\n",
    "#TODO:\n",
    "#1:Increase the size of the dataset: A larger dataset may help the model learn more diverse patterns and improve its accuracy.\n",
    "\n",
    "#2:Adjust the learning rate: The learning rate determines the step size at which the model is updated during training. If it is too high or too low, the model's accuracy may suffer. Experiment with different learning rates to find the optimal one.\n",
    "\n",
    "#3:Add more layers to the model: A deeper network with more layers may help the model learn more complex features and improve its accuracy.\n",
    "\n",
    "#4:Use pre-trained models: Transfer learning can be a useful technique for improving the accuracy of a model. You can use pre-trained models and fine-tune them for your specific task.\n",
    "\n",
    "#5:Perform data augmentation: Data augmentation involves applying transformations to the existing data to create new examples. This can help the model learn more robust features and improve its accuracy.\n",
    "\n",
    "#6:Regularization: Regularization techniques like dropout or weight decay can help prevent overfitting and improve the generalization performance of the model.\n",
    "\n",
    "#7:Hyperparameter tuning: Experiment with different hyperparameters like batch size, number of epochs, optimizer, etc. to find the optimal set of hyperparameters for your model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
